[buildout]
extends =
		base_sources.cfg
		environment.cfg
		nltk.cfg
		jq.cfg
		redis.cfg
		sites.cfg
		reports.cfg
		metadata.cfg
		recorder.cfg
		pillow.cfg
		products.cfg
		memcached.cfg
		publishing.cfg
		asynchronous.cfg
		buildout.cfg

parts +=
	  cython
	  lxml
	  checkversions
	  passwords
	  zcml
	  jq
	  pillow
	  memcached
	  redis
	  eggs
	  main-conf
	  redis-conf
	  pserve-conf
	  gunicorn-conf
	  library-conf
	  site-assets-conf
	  webapp-conf
	  clean-pyc
	  roles
	  nltk
	  nltk_data
	  stripe-conf
	  google-sso-conf
	  dataserver-conf

[server-versions]
# This section can be used to define version pins
# for mr.developer. Currently, we define one version
# pin for all components, as that's the safest way to be sure
# we have a matching set of components that work together.
# We also only define a version pin for the SVN repository,
# as the git components are non-critical (not used at runtime)
# or slow-changinge
# All = @33001
All =
sites =
plasTeX =
rendering =
dataserver =
geventwebsocket =

[dataserver-base-versions]
app.client_preferences = branch=master
app.site = branch=master
site_license = branch=master
app.site_license = branch=master
dataserver = branch=master
identifiers = branch=master
links = branch=master
mailer = branch=master
monkey = branch=master
utils = branch=master

[dataserver-base-sources]
nti.geventwebsocket = git git@github.com:NextThought/nti.geventwebsocket.git ${server-versions:geventwebsocket}
nti.utils = git git@github.com:NextThought/nti.utils.git ${dataserver-base-versions:utils}
nti.identifiers = git git@github.com:NextThought/nti.identifiers.git ${dataserver-base-versions:identifiers}
nti.mailer = git git@github.com:NextThought/nti.mailer.git ${dataserver-base-versions:mailer}

# Dataserver
nti.dataserver = git git@github.com:NextThought/nti.dataserver.git ${dataserver-base-versions:dataserver}
nti.app.client_preferences = git git@github.com:NextThought/nti.app.client_preferences.git ${dataserver-base-versions:app.client_preferences}

# links
nti.links = git git@github.com:NextThought/nti.links.git ${dataserver-base-versions:links}

# Site
nti.app.site = git git@github.com:NextThought/nti.app.site.git ${dataserver-base-versions:app.site}
nti.site_license = git git@github.com:NextThought/nti.site_license.git ${dataserver-base-versions:site_license}
nti.app.site_license = git git@github.com:NextThought/nti.app.site_license.git ${dataserver-base-versions:app.site_license}

# Monkey patch
nti.monkey = git git@github.com:NextThought/nti.monkey.git ${dataserver-base-versions:monkey}

[dataserver-sources]
<= 	dataserver-base-sources
	async-ALL-sources
	metadata-ALL-sources
	reports-ALL-sources
	recorder-ALL-sources
	publishing-ALL-sources

[cython]
recipe = zc.recipe.egg
# Need to include cython so it gets
# on the path and can be used by lxml
# and gevent. This needs to happen
# in a separate part so the scripts
# exist.
eggs =
	cython

[lxml:macosx]
# we rely on the macports lxml
recipe = z3c.recipe.mkdir
paths = ${buildout:parts-directory}/lxml

[lxml:linux]
# A build of lxml that uses local libxml2 and libxslt
# for repeatability and to better work without
# having -dev packages installed.
recipe = z3c.recipe.staticlxml
# If this exact version was already present
# in buildout-eggs, it won't be rebuilt
egg = lxml==4.6.2
libxml2-url = http://xmlsoft.org/sources/libxml2-2.9.8.tar.gz
libxslt-url = http://xmlsoft.org/sources/libxslt-1.1.32.tar.gz

[gevent]
monitor_thread_enable = False
max_blocking_time = 5

[eggs]
dataserver_egg = nti.dataserver[tools]
# Here we list the common application requirements
# that are used in all environments, but no sites
eggs +=
	pillow
	${:dataserver_egg}
	nti.app.site
	nti.wsgi.cors
	nti.identifiers
	nti.app.pyramid_zope
	nti.app.site_license
	nti.app.client_preferences
	${async-ALL-eggs:eggs}
	${reports-ALL-eggs:eggs}
	${metadata-ALL-eggs:eggs}
	${recorder-ALL-eggs:eggs}
	${publishing-ALL-eggs:eggs}

# Let scripts know where they should reach the dataserver
# It's important to only put the Boto keys in the environment
# if they are defined, as they take highest precedence
# Make sure all these scripts know where their
# home is, and that it is a buildout
initialization =
			   import os
			   os.environ['DATASERVER_DIR_IS_BUILDOUT'] = "1"
			   os.environ['DATASERVER_DIR'] = "${deployment:root-directory}"
			   os.environ['DATASERVER_ETC_DIR'] = "${deployment:etc-directory}"
			   os.environ['DATASERVER_DATA_DIR'] = "${deployment:data-directory}"
			   os.environ['CHAMELEON_CACHE'] = "${deployment:root-directory}/var/caches/chameleon_cache"
			   os.environ['DATASERVER_BUILDOUT_PORT'] = "${environment-dataserver:http_port}"
			   os.environ['GEVENT_RESOLVER'] = "dnspython"
			   os.environ['GEVENT_MONITOR_THREAD_ENABLE'] = "${gevent:monitor_thread_enable}"
			   os.environ['GEVENT_MAX_BLOCKING_TIME'] = "${gevent:max_blocking_time}"
			   os.environ['RELSTORAGE_GEVENT_NO_SWITCH_WHEN_LOCKED'] = "1"
			   os.environ['RELSTORAGE_CP_REPLACEMENT_CHANCE_WHEN_FULL'] = "1"
			   os.environ['RELSTORAGE_CP_REPLACEMENT_BEGIN_CONSIDERING_PERCENT'] = "1"
			   os.environ['RELSTORAGE_CP_REPLACEMENT_CHANCE_WHEN_CLOSE'] = "0"
			   if "${boto:aws_bucket_name}": os.environ['AWS_BUCKET_NAME'] = "${boto:aws_bucket_name}"
			   if "${boto:aws_access_key_id}": os.environ['AWS_ACCESS_KEY_ID'] = "${boto:aws_access_key_id}"
			   if "${boto:aws_secret_access_key}": os.environ['AWS_SECRET_ACCESS_KEY'] = "${boto:aws_secret_access_key}"
			   ${:extra-init}
extra-init =

[passwords]
recipe = nti.recipes.passwords
# Real environments will specify the
# file = argument, pointing to a .cast5
# file stored in source control. All the
# settings in that file that have the same name
# as this section will be added to this section
# and available for use in other parts.
sql_passwd =
smtp_passwd =
aws_secret_access_key =
jwt_secret =

[environment]
# See the passwords recipe
sql_passwd = ${passwords:sql_passwd}
smtp_passwd = ${passwords:smtp_passwd}

### StatsD
statsd_host = 127.0.0.1
statsd_port = 8125
statsd_prefix = os.environ.get('HOSTNAME') or ''

[boto]
# Boto configuration
# Note that when using Amazon SES, this is NEVER
# the same thing as the SMTP username; the SMTP
# username is entirely different
aws_bucket_name =
aws_access_key_id =
aws_secret_access_key = ${passwords:aws_secret_access_key}

[roles]
recipe = z3c.recipe.filetemplate
source-directory = templates
dest-directory = ${deployment:root-directory}
force-overwrite = true
files = 666-roles.zcml

[zcml]
# XXX: This section is officially deprecated now;
# it should only be used to add features, not packages
# or pyramid---those are handled with z3c.autoinclude
# You can use the <= syntax to pull in desired features

recipe = nti.recipes.zcml
deployment = deployment

package_zcml =

package_location = package-includes

package_features =
			in-buildout
			forums

pyramid_zcml =

pyramid_location = pyramid-includes
pyramid_file = pyramid
pyramid_features = ${:package_features}

[dataserver-signer]
secret = ${passwords:dataserver_signer_secret}
salt = nti-dataserver

[google-oauth]
authorization_url = https://oauth.nextthought.com/google/oauth1

[dataserver-conf]
recipe = z3c.recipe.filetemplate
source-directory = templates
dest-directory = ${deployment:root-directory}
files = 777-nti.dataserver.zcml

[main-conf]
recipe = z3c.recipe.filetemplate
source-directory = templates
dest-directory = ${deployment:root-directory}
force-overwrite = true
files = main.ini

redis_client_url = file://${deployment:run-directory}/redis.sock

# memcache settings
memcached_servers = ${environment:cache_servers}

[pserve-conf]
recipe = z3c.recipe.filetemplate
# The recipe doesn't take into account
# variables from other sections used in the template
# when determining if it needs to update. Using
# extends seems to fix this
source-directory = templates
force-overwrite = true
dest-directory = ${deployment:root-directory}
files = pserve.ini
interpreted-options =
	statsd_prefix

### Settings for the dataserver
debug_error_in_html = false
dataserver_log_level = DEBUG
analytics_log_level = DEBUG
completion_log_level = DEBUG
retry_attempts = 3
retry_sleep_ms = 50
# This is unused?
deploy_root = ${environment:global_content_directory}
mail_queue_path = ${deployment:mail-directory}
email_error_subject_prefix = Error
email_error_from = automated.emails@nextthought.com
email_error_to = server-errors@nextthought.com
email_default_from = no-reply@nextthought.com
cookie_secret = ${environment:global_host_name}
session_cookie_secret = ${environment:global_host_name}

### StatsD
statsd_host = ${environment:statsd_host}
statsd_port = ${environment:statsd_port}
statsd_prefix = ${environment:statsd_prefix}

### GC
gc_debug = False
gc_disable = False
gc_gen0_threshold = 700
gc_gen1_threshold = 10
gc_gen2_threshold = 10

### Sessions
# How old inactive sessions have to be before they are cleaned up (default 0s)
session_cleanup_age = 0
# If a session is inactive for this long (no pingbacks), we'll consider
# them inactive (default 120s)
session_server_heartbeat_timeout = 120
# How often we update state when a client pings us back. We want to keep db
# churn low if a client is frequently responding to pings (default 60s).
session_reader_heartbeat_update_frequency = 60
# How often the server pings the client (default 5s).
session_ping_frequency = 5

# JWT
jwt_secret = ${passwords:jwt_secret}
jwt_issuer = nextthought

webapp_location = ${environment:webapp_location}
loginapp_location = ${environment:loginapp_location}
invitation_redirect_url = ${environment:invitation_redirect_url}

###
# Extra pyramid configuration
pyramid_includes =

###
# Purchase email confirmations
# For site policies that load the 'send additional purchase confirmation subscriber',
# these addressess will get the emails.
# NOTE: This falls down if multiple sites in the same environment
# need to get different purchase confirmations.
# A whitespace separated list
###
purchase_additional_confirmation_addresses =

###
# Concurrent enrollment email addresses
# For site policies that load the 'send additional enrollment confirmation subscriber',
# these addressess will get the emails.
# A whitespace separated list
###
concurrent_enrollment_addresses = nti-testing@nextthought.com

###
# Invitation BCC email addresses
# List of emails to be BCCed when sending invitation emails
# A comma separated list
###
invitations_bcc =

###
# Enrollment BCC for i2
# A whitespace separated list
###
i2_enrollment_addresses =

# Flag to send emails to non-NT addresses
email_externally = false

## Use secure settings by default; development
# environments need to specifically opt-out of this.
secure_cookies = 1

[gunicorn-conf]
recipe = z3c.recipe.filetemplate
source-directory = templates
force-overwrite = true
dest-directory = ${deployment:root-directory}

files = gunicorn.conf.py

http-port = ${environment-dataserver:http_port}
# A unix socket can be used by nginx for its 'upstream'
# more efficiently than even loopback
unix-socket = ${environment-dataserver:unix-socket}

# For deployment, preloading is awesome. But for those hacking on the
# python code, it interfers with running a debugger in the sub-process
# since it doesn't exec(). This is a python literal
preload_app = True
reuse_port = true
# Auto-calculate by default
workers =
timeout = 1800
# Leave blank for trusted environments
forwarded_allow_ips =

# The maximum number of requests a worker will process before
# restarting. Any value greater than zero will limit the number of
# requests a work will process before automatically restarting. This
# is a simple method to help limit the damage of memory leaks. If this
# is set to zero (the default) then the automatic worker restarts are
# disabled.
max_requests = 0

# The maximum number of simultaneous clients. This setting only
# affects the Eventlet and Gevent worker types.
# If set to the default, 1000, we restrict to 100 in the code.
worker_connections = 250

# Sets the maximum number of consecutive accepts that a process may perform on
# a single wake up. High values give higher priority to high connection rates,
# while lower values give higher priority to already established connections.
# Default is 1 for envs with more than worker.
# https://github.com/benoitc/gunicorn/pull/2266
# Set one to allow more parallelization
raw_env =
	'gevent_blocking_trace=False',
	'gevent_max_blocking_time=5',
	'gevent_max_accept=1'

[library-conf]
recipe = z3c.recipe.filetemplate
source-directory = templates
force-overwrite = true
dest-directory = ${deployment:root-directory}
files = library.zcml

library-type = filesystemLibrary
library-args =
			 directory="${environment:global_content_directory}"
			 prefix="${environment:global_content_location}"

[site-assets-conf]
recipe = z3c.recipe.filetemplate
source-directory = templates
force-overwrite = true
dest-directory = ${deployment:root-directory}
files = site_assets.zcml

site-assets-args = directory="${environment:global_site_assets_directory}"
                   prefix="${environment:global_site_assets_location}"

[webapp-conf]
recipe = z3c.recipe.filetemplate
source-directory = templates
dest-directory = ${deployment:root-directory}
files = config.js

# The list of features and whether they are on or
# not. The last one should not be followed by a comma.
# Probably best to replace this entirely per-environment
# than extend it.
features =
 		 "mutable-forums":true,
		 "fancy-scroll":true,
		 "notepad":true,
		 "v3matching":false,
		 "transcript-follow-video":true,
		 "video-settings":true

# We can optionally specificy a kaltura uiconf id which is
# nice for testing features outside of prod (blank by default
# which means the one hardcoded in the app)
kalturaUIID =

# The location to redirect to when
# the user is not logged in
unauth_location = ${environment:loginapp_location}

# prevent usernames from displaying
# in the app as plain text
obscure_usernames = false

#Open external PDF Content Card links in a new tab
external_pdf_new_window = false

# Whether mobile Safari is allowed
allow_mobile_safari = false

# A string hash value for CloudFront
# CORS caching issues. Environment specific.
cors_salt =  ${environment:global_host_name}

# The master switch to remove user profile
# support. Set to true in COPPA environments
disable_profiles = false

# Controls the presence of a global onerror
# handler to trap uncaught errors during load.
# on by default but useful to turn off in dev environments
enable_global_onerror = true

# Controls the console output mode. (On or Off)
enable_logging = true

# The amount of time a user can be inactive before we deem
# their session has ended.  I.E if a user walks away
# from an open browser tab at what time does the existing session
# end.  Defaults to 900 seconds (15 minutes)
activity_inactive_timeout = 900

# The amount of time a user can be inactive before we warn
# their session is about to end. Defaults to (12 minutes)
activity_inactive_warn = 720

# The amount of time after a users tab is blurred before
# there session is considered inactive.  Defaults to 900 seconds (15 minutes)
activity_blur_timeout = 900

# The amount of time after a users tab is blurred before they are warned that there session
# is about to end.  Defaults to 0 (don't warn)
activity_blur_warn = 0

[crashmail_macro]
program =
supervisor=crashmail_${:program} PROCESS_STATE_EXITED ${deployment:bin-directory}/crashmail [-p ${:program} -m ${pserve-conf:email_error_to} -o ${pserve-conf:email_error_subject_prefix} ]

[crashmail_pserve]
<= crashmail_macro
program = pserve

[crashmail_scheduled_job_executor]
<= crashmail_macro
program = scheduled_job_executor

[crashmail_scheduled_job_dispatcher]
<= crashmail_macro
program = scheduled_job_dispatcher

[fatalmail]
# NOTE: You may need to batch process_state_email_monitor.py
# if you get an error "expected STARTTLS first" to add 's.starttls()'
# NOTE: If some of these things aren't configured, but
# you don't override this, expect fatalmail to not get started
# and expect error messages from supervisor
supervisor=fatalmail PROCESS_STATE,TICK_60 ${deployment:bin-directory}/fatalmailbatch [-t ${pserve-conf:email_error_to} -f ${pserve-conf:email_error_from} -s ${pserve-conf:email_error_subject_prefix} -H ${environment:smtp_server}:${environment:smtp_port} -u ${environment:smtp_username} -p ${passwords:smtp_passwd}]

[supervisor]
recipe = collective.recipe.supervisor
plugins =
		superlance
		mr.laforge
# mr.laforge allows us to say 'supervisorctl kill HUP nginx' https://pypi.python.org/pypi/mr.laforge/0.6
# superlance is where the crash listeners come from http://superlance.readthedocs.org/en/latest/
d_plugins = mr.laforge
ctl_plugins = mr.laforge
rpcplugins = laforge mr.laforge.rpcinterface:make_laforge_rpcinterface
ctlplugins = laforge mr.laforge.controllerplugin:make_laforge_controllerplugin

supervisord-conf = ${deployment:etc-directory}/supervisord.conf
http-socket = unix
file = ${deployment:run-directory}/supervisord.sock
serverurl = unix:///${:file}
childlogdir = ${deployment:log-directory}
logfile = ${deployment:log-directory}/supervisord.log
pidfile = ${deployment:run-directory}/supervisord.pid
loglevel = info
supervisord-environment =
						DATASERVER_DIR=${deployment:root-directory},PYTHONHASHSEED=random

pserve-ini = ${deployment:etc-directory}/pserve.ini
metadata-opts = -v

eventlisteners =
			   ${crashmail_pserve:supervisor}
			   ${crashmail_scheduled_job_executor:supervisor}
			   ${crashmail_scheduled_job_dispatcher:supervisor}
			   ${fatalmail:supervisor}

programs =
		 ${redis-conf:redis-supervisor}
		 ${memcached-conf:memcached-supervisor}
		 999 pserve ${deployment:root-directory}/bin/nti_pserve [${:pserve-ini}]
		 99 metadata ${deployment:bin-directory}/nti_metadata_processor [${:metadata-opts}]

# This option lists the programs that must be running
# for the server to be considered running. In general,
# anything that uses the server code should be listed here.
# entries are comma separated
pserve_group_programs = pserve,metadata
groups =
	   999 server ${:pserve_group_programs}

[qp-cron]
# This recipe installs cronjobs for the current user
# In this case, to run the queue processor.
# NOTE: The command winds up as the subject of an email
# from cron if there's an error. If we're using plain qp,
# it has the password. Should we write a shell script and run that?
# Alternativly, qp makes mention of some .ini file
# When using plain qp and SMTP, use port 587, port 25 may be throttled.
#
# Using nti_qp talks directly to boto, so the boto keys must be correctly
# configured
recipe = z3c.recipe.usercrontab
times = */1 * * * *
command = ${deployment:bin-directory}/nti_qp ${deployment:mail-directory}

[memcache-mon-cron]
recipe = z3c.recipe.usercrontab
times = */7 * * * *
command = conn=`echo stats |socat tcp-connect:${environment:cache_servers} stdio | grep curr_connections | awk '{print $3}' | python -c 'import sys; print sys.stdin.readline().strip()'`; if [ "$conn" -gt "17000" ]; then echo "WARNING: $conn memcache connections"; fi

[translations]
# Update the message catalogs as appropriate
recipe = collective.recipe.cmd
on_install = true
on_update = true
cmds = for d in sources/*; do echo $d; test -f $d/babel.cfg && (cd $d && python ./setup.py extract_messages && python ./setup.py update_catalog && python ./setup.py compile_catalog); done

[ropeproject]
# Create a project file for rope containing all
# the eggs explicitly. This is not enabled by
# default because the .ropeprojects/config.py cannot
# be customized correctly. However, it is easy to get
# a decent path using omelette
recipe = collective.recipe.ropeproject
eggs = ${eggs:eggs}

[checkversions]
# Provides a 'checkversions' script
# that does the same thing as pip-review
# or pip list --outdated but for a buildout
recipe = zc.recipe.egg
eggs = z3c.checkversions [buildout]

[clean-pyc]
# Having .pyc or .pyo files around in develop eggs
# is dangerous if the source file was deleted. It
# can lead to things apparently working (e.g., imports,
# ZODB objects) that really won't on a fresh checkout.
recipe = collective.recipe.cmd
on_install = true
on_update = true
cmds = find ${buildout:sources-dir} -name "*.py[cod]" -delete

[omelette]
recipe = collective.recipe.omelette
eggs = ${eggs:eggs}
# By creating a flattened, non-namespaced directory of all installed
# packages, Omelette makes it easy to browse all code in one place. it
# also makes it easy to set a python path for PyDev and the like.
